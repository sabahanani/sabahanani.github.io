[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sabahanani.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Lisa’s Vegetable Garden",
    "section": "",
    "text": "The Tidytuesday challenge published on Monday 20/05/2024 had data on Lisa Lendway’s vegetable garden.\nThere were 6 data sets in total:\nplanting_2020, planting_2021\nharvest_2020, harvest_2021\nspending_2020, spending_2021"
  },
  {
    "objectID": "TidyTuesday1.html#tidytuesday",
    "href": "TidyTuesday1.html#tidytuesday",
    "title": "Lisa’s Vegetable Garden",
    "section": "",
    "text": "The Tidytuesday challenge published on Monday 20/05/2024 had data on Lisa Lendway’s vegetable garden.\nThere were 6 data sets in total:\nplanting_2020, planting_2021\nharvest_2020, harvest_2021\nspending_2020, spending_2021"
  },
  {
    "objectID": "TidyTuesday1.html#the-data-sets",
    "href": "TidyTuesday1.html#the-data-sets",
    "title": "Lisa’s Vegetable Garden",
    "section": "The Data Sets",
    "text": "The Data Sets\nplanting_2020 & planting_2021:\nThese data sets contained the following variables: vegetable, variation, number of seeds planted, and date.\nharvest_2020 & harvest_2021:\nThese data sets contained the following variables: vegetable, variation, date, weight, and units.\nspending_2020 & spending_2021:\nThese data sets contained the following variables: vegetable, variation, brand, and price (with and without tax)."
  },
  {
    "objectID": "TidyTuesday1.html#the-research-question",
    "href": "TidyTuesday1.html#the-research-question",
    "title": "Lisa’s Vegetable Garden",
    "section": "The Research Question",
    "text": "The Research Question\nMy exploration question was: what are the vegetables that yield the highest weight (in grams) per seed planted?\nIn other words, how many grams does each seed yield for each vegetable?"
  },
  {
    "objectID": "TidyTuesday1.html#how-was-the-data-explored",
    "href": "TidyTuesday1.html#how-was-the-data-explored",
    "title": "Lisa’s Vegetable Garden",
    "section": "How was the Data Explored?",
    "text": "How was the Data Explored?\nAfter I downloaded the data sets from the official tidytuesday github repository, I merged them by common columns (year, variation, vegetable).\nAfter that, I calculated grams per seed by summing weight and seed count separately, and I divided the overall weight by seed count (grouped by vegetable).\ngrams per seed = overall vegetable weight/overall number of seeds planted\nI decided to visualize the top 3 vegetables that yielded the most weight per seed."
  },
  {
    "objectID": "TidyTuesday1.html#the-code",
    "href": "TidyTuesday1.html#the-code",
    "title": "Lisa’s Vegetable Garden",
    "section": "The Code",
    "text": "The Code\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggimage)\nlibrary(grid)\nlibrary(png)\n#loading the data\nspending_data &lt;- read.csv(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/spending_data.csv\")\nplanted_vegetables &lt;- read.csv(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/planted_vegetables.csv\")\nharvested_vegetables &lt;- read.csv(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/harvested_vegetables.csv\")\n#merging the data\nmerged_data &lt;- merge(planted_vegetables, harvested_vegetables, by = c(\"year\", \"vegetable\", \"variety\"), all = TRUE)\nmerged_data &lt;- merge(merged_data, spending_data, by = c(\"year\", \"vegetable\", \"variety\"), all = TRUE)\nmerged_data &lt;- select(merged_data, -date.x, -date.y, -notes, -eggplant_item_number, -plot,-brand,-variety,-year,-price)\nmerged_data&lt;-na.omit(merged_data)\nsetwd(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday\")\nwrite.csv(merged_data,\"merged_data.csv\", row.names = FALSE)\n#converting the relevant variables to numeric (making sure)\nmerged_data &lt;- merged_data |&gt; \n  mutate(\n    number_seeds_planted = as.numeric(number_seeds_planted),\n    weight = as.numeric(weight),\n    price_with_tax = as.numeric(price_with_tax)  # Ensure this column is numeric too\n  )\n#calculating grams per seed\ngrouped_data &lt;- merged_data |&gt; \n  group_by(vegetable) |&gt; \n  summarise(\n    total_weight = sum(weight),\n    total_seeds = sum(number_seeds_planted),\n    .groups = \"drop\"  \n  ) |&gt; \n  mutate(\n    grams_per_seed = total_weight / total_seeds,\n  )\n#top 3 vegetables by grams_per_seed\ntop_vegetables &lt;- grouped_data |&gt; arrange(desc(grams_per_seed)) |&gt; head(3) |&gt; arrange(grams_per_seed)\n#arranging\nvegetable_order &lt;- top_vegetables$vegetable\ntop_vegetables &lt;- top_vegetables |&gt; mutate(vegetable = factor(vegetable, levels = vegetable_order))\n#images to replace bars, and adding images as labels\npics&lt;-c(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/IMG_5578-removebg-preview.png\",\n        \"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/Screenshot_2024-05-31_at_13.20.05-removebg-preview.png\",\n        \"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/Screenshot_2024-05-31_at_13.20.26-removebg-preview.png\"\n        )\nlabelpics&lt;-c(\"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/407-removebg-preview (1).png\",\n             \"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/576-removebg-preview.png\",\n             \"/Users/sabahanani/Desktop/תואר שני/Semester B/מעבדה במדעי הנתונים ופסיכולוגיה/tidytuesday/finalimages/618-removebg-preview.png\")\n#image sizing\nimage_rasters &lt;- lapply(pics, function(path) rasterGrob(readPNG(path), interpolate = TRUE))\n#plotting\np &lt;- ggplot(top_vegetables, aes(x = vegetable, y = grams_per_seed)) +\n  geom_bar(stat = \"identity\", fill = \"transparent\") +\n  scale_y_continuous(limits = c(0, 800), breaks = seq(0, 800, by = 50)) +\n  labs(title = \"Top Crops by Average Crop Yield\", \n       subtitle = \"(Grams harvested per seed)\",\n       x = \"Crop\", y = \"Crop Yield (in grams)\") +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"turquoise4\"),\n    plot.background = element_rect(fill = \"turquoise4\"),\n    plot.title = element_text(color = \"white\", hjust = 0.5),\n    plot.subtitle = element_text(color = \"white\", hjust = 0.5),\n    axis.text = element_text(color = \"white\"),\n    axis.title = element_text(color = \"white\"),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) + \n  geom_image(aes(image = labelpics), size = 0.3, by = \"width\") \n#adding custom annotations for each image, adjusting on y-axis\nfor (i in seq_along(image_rasters)) {\n  p &lt;- p + annotation_custom(\n    grob = image_rasters[[i]], \n    xmin = i - 0.4, xmax = i + 0.4,\n    ymin = -50, ymax = top_vegetables$grams_per_seed[i]+12\n  )\n}\n#viewing the plot\np &lt;- p + labs(\n  caption = paste(\"Source: Lisa's Vegetable Garden, tidytuesday repo\", \"\\nAI art generated by Microsoft's Copilot\", sep = \"\"),\n  x = \"Crop\", y = \"Crop Yield (in grams)\"\n) +\n  theme(\n    plot.caption = element_text(color = \"white\")  \n  )\n# Viewing the plot\np"
  },
  {
    "objectID": "TidyTuesday1.html#results",
    "href": "TidyTuesday1.html#results",
    "title": "Lisa’s Vegetable Garden",
    "section": "Results",
    "text": "Results\nThe three top vegetables were zucchini, tomatoes, and pumpkins. Pumpkins yielded about 618 grams per seed, tomatoes yielded about 576 grams per seed, and zucchinis yielded about 407 grams per seed.\nHowever, there are factors that were not taken into account.\n\nFactors that Could Influence the Data\nEnvironmental factors: soil quality, temperature, moisture, and sun exposure.\nVegetable variation: the exploration question did not take the vegetable variation into account."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Tidytuesday of May 20, 2024\n\n\n\n\n\n\n\nSQL Class Assignment\n\n\n\n\n\n\n\nInteractive PowerBI Dashboard\n\n\n\n\n\n\n\nRStudio Statistical Analysis, Heart Attack Risk Prediction\n\n\n\n\n\n\n\nfMRI Data Analysis with Python"
  },
  {
    "objectID": "Assignment_Saba.html",
    "href": "Assignment_Saba.html",
    "title": "SQL Class Assignment",
    "section": "",
    "text": "You are a data scientist at Strauss-Elite.\nYour boss hands you a data request by a client. You said you would get to it the next day - but that night you went out partying, and when you awoke the next morning you found the following query typed out on your computer, having no recollection of writing it, nor can you find the client’s request.\nWITH total_orders AS (\nSELECT cid, SUM(Price * amount) AS sum_total\nFROM `orders`\nLEFT JOIN products USING (pid)\nWHERE (sale_date &gt; '2023-01-01')\nGROUP BY cid\n)\nSELECT *\nFROM total_orders\nLEFT JOIN customers USING (cid)\nORDER BY sum_total DESC\nLIMIT 1\n\n1. Assuming you got it right - what did the client want? Explain the query.\nTo understand the query, we must break it down by working inside out, then explain it line by line (or clause by clause).\nThe query starts with WITH, which is a Common Table Expression. What WITH does is make nested queries more readable. It gives temporary names to each query instead of nesting the queries together in a complex way. This makes the code more manageable.\n\nLet’s break down the first sub-query:\nIn the first line, a certain query is named “total_orders.”\nLEFT JOIN products USING (pid) WHERE (sale_date &gt; '2023-01-01')\nFirst of all, before we explain the SELECT statement, we have to understand where we are extracting, or SELECTing, from.\nWe are left joining the orders table with another table called products. What LEFT JOIN does is join all the relevant rows from the left table (in this case orders), with another table using certain criteria. Here, we are using the column pid (product id) to join the two tables. The join will be done using rows from the orders table in the pid column, which have matching values in the pid column in the products table.\nHowever, this join is filtered with WHERE. We are left joining the two tables on the condition that the sale_date (the date where the sale was made) column in the orders table has values bigger than 2023-01-01.\nSo, we are left joining the matching rows in the orders table with the products table based on the pid column, focusing on rows that have values more than 2023-01-01.\n\nSELECT cid, SUM(Price * amount) AS sum_total\nIn this line, we are selecting the column cid (customer id, found in the orders table). We are also selecting the columns Price (price of the candy bar, from the products table) and amount (amount of candy bars purchased, from the orders table), multiplying them (each row with the corresponding row), then summing the multiplication product for the corresponding cid. This sum of multiplications is named sum_total.\nThese columns are extracted from the joining of the two tables.\nAnd as a whole:\n\nWITH total_orders AS ( SELECT cid, SUM(Price * amount) AS sum_total FROM orders LEFT JOIN products USING (pid) WHERE (sale_date &gt; '2023-01-01') GROUP BY cid )\n\nThis sub-query, previously named total_orders with the WITH statement, left joins the tables orders and products based on the pid column. For each cid (GROUP BY cid), a sum_total is calculated, and the results that are shown are those after the date of January 1st, 2023.\n\nNow, let’s break down the second sub-query:\n\nLEFT JOIN customers USING (cid)\nAs said before, before we understand the SELECT statement, we have to know where we are selecting from. Here, we are left joining total_orders with a new table called customers, based on matching rows in the cid column.\nSELECT * FROM total_orders\nNow, we are selecting all columns from the joining of these two tables.\nORDER BY sum_total DESC\nThis line orders the results by the sum_total column in descending order. The highest values will be shown first.\nLIMIT 1\nHere, we are limiting the rows to one. This means that only one row will be shown. Since the results are shown in descending order based on the sum_total column, this means that the highest value will be shown.\nAnd, as a whole, again:\nSELECT * FROM total_orders LEFT JOIN customers USING (cid) ORDER BY sum_total DESC LIMIT 1\nThis sub-query, in summary, left joins the total_orders table with the customers table using the cid column. It orders the table based on the sum_total column in descending order, and only the row with the highest sum_total value will be extracted.\nTaking these explanations into account, now we can sum up the whole query:\nIn the first sub-query, we are left joining the orders table with the products table. Then we calculate, for each customer, the amount of money (sum_total) they spent on candy bars by multiplying the price of the candy bar by the amount purchased. This is only done for orders that were made after January 1st, 2023. This whole extraction is now called total_orders.\nIn the second sub-query, we are left joining the total_orders table with the customers table, orders the results in descending order based on how much money was spent, and shows the customer with the highest sum_total value.\nTherefore, we can say that, essentially, this query extracts the customer id (cid) that spent the most amount of money (sum_total) on candy bars after January 1st, 2023.\n\n2. Run the query - what is the answer?\n\nTo run the query, we must first set up a fake database connection.\n\nlibrary(DBI)\nlibrary(RSQLite)\ncon_chocolate &lt;- dbConnect(drv = SQLite(),\ndbname = \"/Users/sabahanani/Desktop/Projects/sabahanani.github.io/chocolate.sqlite\")\n\nNow, we can run the query.\n\nWITH total_orders AS (\nSELECT cid, SUM(Price * amount) AS sum_total\nFROM `orders`\nLEFT JOIN products USING (pid)\nWHERE (sale_date &gt; '2023-01-01')\nGROUP BY cid\n)\nSELECT *\nFROM total_orders\nLEFT JOIN customers USING (cid)\nORDER BY sum_total DESC\nLIMIT 1\n\n\n1 records\n\n\ncid\nsum_total\nName\nAddress\n\n\n\n\n822\n1057.03\nBeieler, Joshua\n1866 Smith St., Jacobson, Michigan\n\n\n\n\n\nAs we can see, the output of the query is the customer who spent the most on candy bars: his id (822), the sum he spent (1057.03), his name (Joshua Beieler), and his address (1866 Smith St., Jacobson, Michigan)."
  },
  {
    "objectID": "Assignment_Saba.html#q1",
    "href": "Assignment_Saba.html#q1",
    "title": "SQL Class Assignment",
    "section": "",
    "text": "You are a data scientist at Strauss-Elite.\nYour boss hands you a data request by a client. You said you would get to it the next day - but that night you went out partying, and when you awoke the next morning you found the following query typed out on your computer, having no recollection of writing it, nor can you find the client’s request.\nWITH total_orders AS (\nSELECT cid, SUM(Price * amount) AS sum_total\nFROM `orders`\nLEFT JOIN products USING (pid)\nWHERE (sale_date &gt; '2023-01-01')\nGROUP BY cid\n)\nSELECT *\nFROM total_orders\nLEFT JOIN customers USING (cid)\nORDER BY sum_total DESC\nLIMIT 1\n\n1. Assuming you got it right - what did the client want? Explain the query.\nTo understand the query, we must break it down by working inside out, then explain it line by line (or clause by clause).\nThe query starts with WITH, which is a Common Table Expression. What WITH does is make nested queries more readable. It gives temporary names to each query instead of nesting the queries together in a complex way. This makes the code more manageable.\n\nLet’s break down the first sub-query:\nIn the first line, a certain query is named “total_orders.”\nLEFT JOIN products USING (pid) WHERE (sale_date &gt; '2023-01-01')\nFirst of all, before we explain the SELECT statement, we have to understand where we are extracting, or SELECTing, from.\nWe are left joining the orders table with another table called products. What LEFT JOIN does is join all the relevant rows from the left table (in this case orders), with another table using certain criteria. Here, we are using the column pid (product id) to join the two tables. The join will be done using rows from the orders table in the pid column, which have matching values in the pid column in the products table.\nHowever, this join is filtered with WHERE. We are left joining the two tables on the condition that the sale_date (the date where the sale was made) column in the orders table has values bigger than 2023-01-01.\nSo, we are left joining the matching rows in the orders table with the products table based on the pid column, focusing on rows that have values more than 2023-01-01.\n\nSELECT cid, SUM(Price * amount) AS sum_total\nIn this line, we are selecting the column cid (customer id, found in the orders table). We are also selecting the columns Price (price of the candy bar, from the products table) and amount (amount of candy bars purchased, from the orders table), multiplying them (each row with the corresponding row), then summing the multiplication product for the corresponding cid. This sum of multiplications is named sum_total.\nThese columns are extracted from the joining of the two tables.\nAnd as a whole:\n\nWITH total_orders AS ( SELECT cid, SUM(Price * amount) AS sum_total FROM orders LEFT JOIN products USING (pid) WHERE (sale_date &gt; '2023-01-01') GROUP BY cid )\n\nThis sub-query, previously named total_orders with the WITH statement, left joins the tables orders and products based on the pid column. For each cid (GROUP BY cid), a sum_total is calculated, and the results that are shown are those after the date of January 1st, 2023.\n\nNow, let’s break down the second sub-query:\n\nLEFT JOIN customers USING (cid)\nAs said before, before we understand the SELECT statement, we have to know where we are selecting from. Here, we are left joining total_orders with a new table called customers, based on matching rows in the cid column.\nSELECT * FROM total_orders\nNow, we are selecting all columns from the joining of these two tables.\nORDER BY sum_total DESC\nThis line orders the results by the sum_total column in descending order. The highest values will be shown first.\nLIMIT 1\nHere, we are limiting the rows to one. This means that only one row will be shown. Since the results are shown in descending order based on the sum_total column, this means that the highest value will be shown.\nAnd, as a whole, again:\nSELECT * FROM total_orders LEFT JOIN customers USING (cid) ORDER BY sum_total DESC LIMIT 1\nThis sub-query, in summary, left joins the total_orders table with the customers table using the cid column. It orders the table based on the sum_total column in descending order, and only the row with the highest sum_total value will be extracted.\nTaking these explanations into account, now we can sum up the whole query:\nIn the first sub-query, we are left joining the orders table with the products table. Then we calculate, for each customer, the amount of money (sum_total) they spent on candy bars by multiplying the price of the candy bar by the amount purchased. This is only done for orders that were made after January 1st, 2023. This whole extraction is now called total_orders.\nIn the second sub-query, we are left joining the total_orders table with the customers table, orders the results in descending order based on how much money was spent, and shows the customer with the highest sum_total value.\nTherefore, we can say that, essentially, this query extracts the customer id (cid) that spent the most amount of money (sum_total) on candy bars after January 1st, 2023.\n\n2. Run the query - what is the answer?\n\nTo run the query, we must first set up a fake database connection.\n\nlibrary(DBI)\nlibrary(RSQLite)\ncon_chocolate &lt;- dbConnect(drv = SQLite(),\ndbname = \"/Users/sabahanani/Desktop/Projects/sabahanani.github.io/chocolate.sqlite\")\n\nNow, we can run the query.\n\nWITH total_orders AS (\nSELECT cid, SUM(Price * amount) AS sum_total\nFROM `orders`\nLEFT JOIN products USING (pid)\nWHERE (sale_date &gt; '2023-01-01')\nGROUP BY cid\n)\nSELECT *\nFROM total_orders\nLEFT JOIN customers USING (cid)\nORDER BY sum_total DESC\nLIMIT 1\n\n\n1 records\n\n\ncid\nsum_total\nName\nAddress\n\n\n\n\n822\n1057.03\nBeieler, Joshua\n1866 Smith St., Jacobson, Michigan\n\n\n\n\n\nAs we can see, the output of the query is the customer who spent the most on candy bars: his id (822), the sum he spent (1057.03), his name (Joshua Beieler), and his address (1866 Smith St., Jacobson, Michigan)."
  },
  {
    "objectID": "Assignment_Saba.html#q2",
    "href": "Assignment_Saba.html#q2",
    "title": "SQL Class Assignment",
    "section": "Q2",
    "text": "Q2\nIn a single query, find:\n• Who is the sales rep that made the larger number of sales compared to the average of all sales reps that were recruited in the same year as him? How many sales?\n• Who is their top customer (with the largest purchase)?\n• What is the sales rep most sold candy bar?\n\n(Supply both the answer and the query.)\nFirst of all, let’s write a sub-query to find the average sales of each sales rep compared to the average sales of the sales reps recruited in the same year:\n\nSELECT srid, Name, year_joined, COUNT(amount) AS sales_count, AVG(COUNT(amount)) OVER (PARTITION BY year_joined) AS avg_sales\nFROM salesreps LEFT JOIN orders USING(srid)\nGROUP BY srid\n\n\nDisplaying records 1 - 10\n\n\nsrid\nName\nyear_joined\nsales_count\navg_sales\n\n\n\n\n8\nChen, Alexander\n2008\n148\n155.3333\n\n\n15\nValdez, Nohemi\n2008\n148\n155.3333\n\n\n23\nSchwarzenbach, Joshua\n2008\n170\n155.3333\n\n\n9\nAragon, Ashley\n2009\n171\n166.5000\n\n\n10\nGraves, Blanca\n2009\n162\n166.5000\n\n\n14\nTootle, Naudia\n2010\n170\n166.0000\n\n\n26\nal-Sadri, Saamyya\n2010\n150\n166.0000\n\n\n27\nal-Farrah, Ghaaliba\n2010\n178\n166.0000\n\n\n13\nRegister, Tayanna\n2011\n176\n168.6667\n\n\n20\nKim, Miles\n2011\n164\n168.6667\n\n\n\n\n\nLet’s break down the sub-query line by line:\nFROM salesreps LEFT JOIN orders USING(srid)\n\nIn this line, we are left joining the salesreps table with the orders table, based on the srid column.\n\nSELECT srid, Name, year_joined,COUNT(amount) AS sales_count, AVG(COUNT(amount)) OVER (PARTITION BY year_joined) AS avg_sales\n\nHere, we are selecting the srid, Name, and year_joned columns from the joining of the two tables. In addition, we are extracting the sales count of each sales rep, by using COUNT(amount) (this is given an alias of sales_count). To calculate the average of each sales count in a certain year, AVG(COUNT(amount)) is used, and PARTITIONed over year_joined (later given an alias of avg_sales).\n\nGROUP BY srid\nThe results are then grouped by the sales rep’s id.\n\nNow, let’s find the sales rep with the largest amount of sales compared to the sales reps who were recruited in the same year as them.\nBut first, let’s give the initial sub-query an alias so we can reuse it:\n\nWITH sales_year_avg AS (\n    SELECT srid, Name, year_joined,COUNT(amount) AS sales_count, AVG(COUNT(amount)) OVER (PARTITION BY year_joined) AS avg_sales\n    FROM salesreps LEFT JOIN orders USING(srid)\n    GROUP BY srid\n)\n--now let's select the salesrep with the most sales compared to the avg:\nSELECT *\nFROM sales_year_avg\nWHERE sales_count &gt; avg_sales\nORDER BY (sales_count - avg_sales) DESC\nLIMIT 1\n\n\n1 records\n\n\nsrid\nName\nyear_joined\nsales_count\navg_sales\n\n\n\n\n7\nPryor, Briana\n2018\n177\n159\n\n\n\n\n\nFROM sales_year_avg\n\nWe are extracting from the initial sub-query, now named sales_year_avg. Previously, that sub-query calculated the average of the sales amount, partitioned by year.\n\nSELECT *\n\nNow, we are selecting, from sales_year_avg, all columns.\n\nWHERE sales_count &gt; avg_sales\n\nwe are filtering the rows selected, where the rows that will be extracted are the rows were the sales_count is larger than the avg_sales.\n\nORDER BY (sales_count - avg_sales) DESC LIMIT 1\n\nNow, we are ordering the filtered rows based on the difference between sales_count and avg_sales in descending order. Then, we are limiting the results to one row. Since the rows are in a descending order, this should give us the sales rep with the largest difference between sales_amount and avg_sales.\n\nNow, let’s find the sale’s rep’s top customer (with the largest purchase-according to how many candy bars purchased and not how much they paid):\n\n\nSELECT customers.cid, Name AS customer_name, SUM(amount) AS total_purchase, orders.srid\nFROM customers \nINNER JOIN orders ON customers.cid = orders.cid\nGROUP BY orders.cid\nORDER BY total_purchase DESC\n\n\nDisplaying records 1 - 10\n\n\ncid\ncustomer_name\ntotal_purchase\nsrid\n\n\n\n\n113\nGarcia, Christina\n194\n2\n\n\n275\nLavaka, Ashleigh\n179\n7\n\n\n118\nGreene, Grigoriy\n177\n4\n\n\n129\nDemby, Kyle\n174\n1\n\n\n379\nal-Hashmi, Inaaya\n171\n4\n\n\n50\nBrooks, Cody\n170\n2\n\n\n675\nMack, Magdalena\n169\n6\n\n\n591\nal-Abdul, Zaghlool\n169\n4\n\n\n767\nal-Habibi, Hamdoona\n166\n4\n\n\n753\nel-Muhammed, Marwa\n165\n2\n\n\n\n\n\nFROM customers  INNER JOIN orders ON customers.cid = orders.cid\n\nHere, we are joining the customers table with the orders table using the cid column. The join of these tables will be our new table.\n\nSELECT Name AS customer_name, SUM(amount) AS total_purchase, srid\n\nNow, we are selecting the name of the customer (customer_name), the sum of the amount of candy bars they purchased (SUM(amount)-total_purchase). I gave the customer’s name an alias because I don’t want it to get confused with the sales reps names. Selecting srid will help us find the sales rep later.\nGROUP BY orders.cid To find the amount purchased by each customer, they have to be grouped by customer id.\n\nORDER BY total_purchase\n\nThen, total_purchase is ordered in a descending manner, where the customer with the biggest purchase will be on top.\n\nNow, let’s join them:\n\n\nWITH sales_year_avg AS (\n    SELECT srid, Name, year_joined,COUNT(amount) AS sales_count, AVG(COUNT(amount)) OVER (PARTITION BY year_joined) AS avg_sales\n    FROM salesreps LEFT JOIN orders USING(srid)\n    GROUP BY srid),\n--giving an alias to the top sales rep\ntop_salesrep AS (\n SELECT *\n FROM sales_year_avg\n WHERE sales_count &gt; avg_sales\n ORDER BY (sales_count - avg_sales) DESC\n LIMIT 1\n ),\n--giving an alias to the top customer\ntop_customer AS (\n SELECT Name AS customer_name, SUM(amount) AS total_purchase, orders.srid\n FROM customers \n INNER JOIN orders ON customers.cid = orders.cid\n GROUP BY orders.cid\n ORDER BY total_purchase DESC\n)\n--finding the top customer for the top sales rep\nSELECT *\nFROM top_salesrep INNER JOIN top_customer \nUSING(srid)\nLIMIT 1\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\nsrid\nName\nyear_joined\nsales_count\navg_sales\ncustomer_name\ntotal_purchase\n\n\n\n\n7\nPryor, Briana\n2018\n177\n159\nLavaka, Ashleigh\n179\n\n\n\n\n\nTo find the sales rep most sold candy bar:\n\nSELECT candy_names, SUM(amount), orders.srid\nFROM orders INNER JOIN products \nON orders.pid=products.pid\nGROUP BY candy_names\nORDER BY SUM(amount) DESC\n\n\nDisplaying records 1 - 10\n\n\ncandy_names\nSUM(amount)\nsrid\n\n\n\n\nMarshmallow Dream\n4127\n19\n\n\nCoconut Crave\n4126\n6\n\n\nLemon Zest\n3992\n25\n\n\nBlueberry Blast\n3988\n30\n\n\nFudge Fusion\n3936\n28\n\n\nStrawberry Swirl\n3865\n28\n\n\nToffee Temptation\n3842\n20\n\n\nAlmond Bliss\n3839\n22\n\n\nChoco Blast\n3771\n2\n\n\nCaramel Crunch\n3737\n1\n\n\n\n\n\nFROM orders INNER JOIN products  ON orders.pid=products.pid\n\nFirst, we join the order table with the products table using pid.\n\nSELECT candy_names, SUM(amount), orders.srid\n\nThen, we select the candy_names, how many of each was purchased, and the sales rep id (srid). This will help us find the sales rep later.\n\nGROUP BY candy_names ORDER BY SUM(amount) DESC\n\nLater, we group by candy names because we’re interested in it as a whole category. Then, we order the results in a descending manner so the candy name with the most amount purchased will be shown first.\n\nNow, let’s find the sale’s rep most sold candy bar name by joining:\n\n\nWITH sales_year_avg AS (\n    SELECT srid, Name, year_joined,COUNT(amount) AS sales_count, AVG(COUNT(amount)) OVER (PARTITION BY year_joined) AS avg_sales\n    FROM salesreps LEFT JOIN orders USING(srid)\n    GROUP BY srid),\n--giving an alias to the top sales rep\ntop_salesrep AS (\n SELECT *\n FROM sales_year_avg\n WHERE sales_count &gt; avg_sales\n ORDER BY (sales_count - avg_sales) DESC\n LIMIT 1\n ),\n--giving an alias to the top customer\ntop_customer AS (\n SELECT Name AS customer_name, SUM(amount) AS total_purchase, orders.srid\n FROM customers \n INNER JOIN orders ON customers.cid = orders.cid\n GROUP BY orders.cid\n ORDER BY total_purchase DESC\n),\n--finding the top customer for the top sales rep, giving it an alias:\ntop_sales_customer AS (\n SELECT *\n FROM top_salesrep INNER JOIN top_customer \n USING(srid)\n LIMIT 1\n ),\n--giving an alias to the most sold candy:\ntop_candy AS (\n SELECT candy_names, SUM(amount), orders.srid\n FROM orders INNER JOIN products \n ON orders.pid=products.pid\n GROUP BY candy_names\n ORDER BY SUM(amount) DESC\n)\n--finding the sales rep most sold candy:\nSELECT *\nFROM top_sales_customer INNER JOIN top_candy \nUSING(srid)\nLIMIT 1\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nsrid\nName\nyear_joined\nsales_count\navg_sales\ncustomer_name\ntotal_purchase\ncandy_names\nSUM(amount)\n\n\n\n\n7\nPryor, Briana\n2018\n177\n159\nLavaka, Ashleigh\n179\nCookies ’n Cream\n3672\n\n\n\n\n\nTo sum up, the best performing sales representative compared to the average of the sales made by others recruited in the same year is: Briana Pryor, with a sales count of 177.\nTheir top customer is: Ashleigh Lavaka (found according to how many candy bars she purchased).\nTheir most sold candy bar is: Cookies ’n Cream"
  },
  {
    "objectID": "Heart_Attack_Risk.html",
    "href": "Heart_Attack_Risk.html",
    "title": "Heart Attack Risk Prediction",
    "section": "",
    "text": "The dataset we will be using is called “Heart Attack Prediction,” downloaded from Kaggle. It includes 5,000 observations (randomly sampled and independent) and 26 variables:\n            Patient ID, Age, Sex (categorical), Cholesterol (continuous), Blood Pressure (continuous), Heart Rate (continuous), Diabetes (dummy coded, 0/1), Family History (dummy coded, 0/1), Smoking (dummy coded, 0/1), Obesity (dummy coded, 0/1), Alcohol Consumption (dummy coded, 0/1), Exercise Hours (continuous), Diet (categorical, Healthy/Average/Unhealthy), Previous Heart Problems (dummy coded, 0/1), Medication Use (dummy coded, 0/1), Stress Level (continuous), Sedentary Hours Per Day (continuous), Income (continuous), BMI (body mass index; continuous), Triglycerides (continuous), Physical Activity Days Per Week (discrete), Sleep Hours Per Day (discrete), Country (categorical), Continent (categorical), Hemisphere (categorical), and Heart Attack Risk (dummy coded, 0/1)."
  },
  {
    "objectID": "Heart_Attack_Risk.html#introduction",
    "href": "Heart_Attack_Risk.html#introduction",
    "title": "Heart Attack Risk Prediction",
    "section": "",
    "text": "The dataset we will be using is called “Heart Attack Prediction,” downloaded from Kaggle. It includes 5,000 observations (randomly sampled and independent) and 26 variables:\n            Patient ID, Age, Sex (categorical), Cholesterol (continuous), Blood Pressure (continuous), Heart Rate (continuous), Diabetes (dummy coded, 0/1), Family History (dummy coded, 0/1), Smoking (dummy coded, 0/1), Obesity (dummy coded, 0/1), Alcohol Consumption (dummy coded, 0/1), Exercise Hours (continuous), Diet (categorical, Healthy/Average/Unhealthy), Previous Heart Problems (dummy coded, 0/1), Medication Use (dummy coded, 0/1), Stress Level (continuous), Sedentary Hours Per Day (continuous), Income (continuous), BMI (body mass index; continuous), Triglycerides (continuous), Physical Activity Days Per Week (discrete), Sleep Hours Per Day (discrete), Country (categorical), Continent (categorical), Hemisphere (categorical), and Heart Attack Risk (dummy coded, 0/1)."
  },
  {
    "objectID": "Heart_Attack_Risk.html#method",
    "href": "Heart_Attack_Risk.html#method",
    "title": "Heart Attack Risk Prediction",
    "section": "Method",
    "text": "Method\nWe are going to test four research questions regarding the data on heart attack risk, using RStudio to statistically analyze the data:\n1. Is there a difference in Cholesterol levels between female and male patients?\nThis question will be analyzed in three ways learned in class:\na. Null hypothesis significance testing (NHST): here, the null hypothesis would be that there is no difference between the genders, while the alternative hypothesis would state that there is a difference.\nb. Equivalence test: here, the main question would be if Cholesterol levels for the genders are equivalent within a certain margin of error.\nc. Bayesian Testing: here, we will test the probability that Cholesterol levels for females’ is different than that of males’, given the data and prior beliefs.\nBefore analyzing the data to answer this question, we will conduct a power analysis to determine the sample size needed.\n2. How does Cholesterol levels vary across different combinations of Obesity and Smoking status? This question will be analyzed using a two-way analysis of variance (ANOVA). Our dependent variable will be Cholesterol, while our between-subject variables will be Obesity and Smoking.\n3. Does BMI and Exercise Hours predict Cholesterol levels?\nThis question will be analyzed using a multiple regression model.\n4. Does Exercise Hours Per Week moderate the relationship between Family History and Cholesterol?\nThis question will be analyzed using moderation analysis. In addition to this question, we will also be comparing the moderation model to the additive version to see which is a better fit and splitting the data of each model into training and test sets.\na new package we will be using in this assignment and that we have not learned in class is the “pwr” package. We chose to use this specific package to calculate how many participants are needed in each group to get to a statistical power of 90% in the t-test used in our first research question."
  },
  {
    "objectID": "Heart_Attack_Risk.html#results",
    "href": "Heart_Attack_Risk.html#results",
    "title": "Heart Attack Risk Prediction",
    "section": "Results",
    "text": "Results\nThe First Question\nFirst, we conducted a power analysis to determine the required sample size for a two-sample t-test. The parameters we considered were effect size (Cohen’s d) of 0.2 (bad scenario where there is only a small effect in the real world), a significance level of 0.05, and a desired power of 0.9 under a two-sided alternative hypothesis. The analysis yielded a calculated sample size of approximately 526 participants per group, indicating the number needed to achieve a statistical power of 90%. This ensures a larger likelihood of correctly rejecting the null hypothesis and detecting a meaningful effect size.\nThe t-test results showed that there is no significant difference between male patients (M = 200, SD = 29.76) and female patients (M = 199.56, SD = 30.86) in Cholesterol levels [t(4998) = 0.51, 95% C.I. (-1.23, 2.12),p = .6]. In assessing Cholesterol levels between male (M = 200, SD = 29.76) and female (M = 199.56, SD = 30.86) patients using two one-sided t-tests using equivalence testing, the results indicated that neither the test for the lower bound [t = 0.066, p = 0.47] nor the test for the upper bound [t = -1.099, p = 0.136] reached significance. The equivalence test overall was not significant [t= 0.07, p = 0.473], indicating that the difference in cholesterol levels between male and female groups does not fall within the predefined equivalence interval. Therefore, we cannot reject the null hypothesis that there is a difference in Cholesterol levels between males and females.\nThe Bayesian t-test indicated strong evidence in favor of the null hypothesis (BF=.036). This Bayes Factor suggests that the observed data are much more likely under the null hypothesis compared to the alternative hypothesis. The 95% Highest Density Interval (HDI) for the true difference between groups in cholesterol levels is estimated to be [-2.10, 1.19]. This interval spans both negative and positive differences, suggesting non-significant differences between the groups. The probability distribution (pd) indicates a 69.73% likelihood for the true difference to be slightly higher for a negative difference. Additionally, the Region of Practical Equivalence (ROPE) is defined as [-3.03, 3.03], where differences are considered negligible. Notably, 100% of the posterior distribution falls within this ROPE, indicating strong support for the practical equivalence between the groups. Cohen’s d was estimated to be -0.01 [95% CI (-0.07, 0.04)], suggesting a negligible negative effect size.\n\nlibrary(afex) \nlibrary(emmeans) \nlibrary(effectsize)\nlibrary(ggeffects) \nlibrary(parameters) \nlibrary(performance) \nlibrary(pwr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(correlation)\nlibrary(TOSTER) \nlibrary(BayesFactor) \nlibrary(bayestestR)\ndata&lt;-read.csv(\"/Users/sabahanani/Desktop/תואר שני/Semester A/שיטות מחקר מתקדמות/עבודה מסכמת/206683112_206357170.csv\")\n#NHST, Bayes, & Equivalence test\n#first of all, power analysis\npwr.t.test(d=0.2, sig.level=0.05, power=0.9, alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 526.3332\n              d = 0.2\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n#t-test\nt_test_HR &lt;- t.test(data$Cholesterol[data$Sex == \"Male\"],\n                    data$Cholesterol[data$Sex == \"Female\"], var.equal = TRUE)\nt_test_HR\n\n\n    Two Sample t-test\n\ndata:  data$Cholesterol[data$Sex == \"Male\"] and data$Cholesterol[data$Sex == \"Female\"]\nt = 0.51648, df = 4998, p-value = 0.6055\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.238192  2.123963\nsample estimates:\nmean of x mean of y \n 200.0004  199.5575 \n\n# t-test results for Cholesterol levels between female and male patients:\n# t-value = 0.51, p-value = 0.6, 95% CI: [-1.23, 2.12]\n# Conclusion: No significant difference in cholesterol levels observed between male and female patients.\n# 2. equivalence t-test:\nt_TOST(formula = Cholesterol ~ Sex, data = data,\n       eqb = .5, smd_ci = \"t\", var.equal = TRUE)\n\n\nTwo Sample t-test\n\nThe equivalence test was non-significant, t(4998) = 0.07, p = 0.47\nThe null hypothesis test was non-significant, t(4998) = -0.516p = 0.61\nNHST: don't reject null significance hypothesis that the effect is equal to zero \nTOST: don't reject null equivalence hypothesis\n\nTOST Results \n                  t   df p.value\nt-test     -0.51648 4998   0.606\nTOST Lower  0.06661 4998   0.473\nTOST Upper -1.09957 4998   0.136\n\nEffect Sizes \n           Estimate      SE              C.I. Conf. Level\nRaw        -0.44289 0.85750 [-1.8536, 0.9678]         0.9\nHedges's g -0.01461 0.02828 [-0.0611, 0.0319]         0.9\nNote: SMD confidence intervals are an approximation. See vignette(\"SMD_calcs\").\n\n# Equivalence t-test results for Cholesterol levels between female and male patients:\n# t-value = 0.07, p-value = 0.47\n# TOST Lower: 0.06, p-value = 0.473\n# TOST Upper: -1.09, p-value = 0.13\n# Conclusion: The equivalence test was not significant, supporting the null hypothesis that cholesterol levels between male and female groups are not equivalent.\n# 3. Bayesian: \nBayesian_model &lt;- ttestBF(formula = Cholesterol ~ Sex, data = data)\ndescribe_posterior(Bayesian_model) # difference\n\nSummary of Posterior Distribution\n\nParameter  | Median |        95% CI |     pd |          ROPE | % in ROPE |    BF |              Prior\n-----------------------------------------------------------------------------------------------------\nDifference |  -0.44 | [-2.09, 1.28] | 69.77% | [-3.03, 3.03] |      100% | 0.036 | Cauchy (0 +- 0.71)\n\nhdi(Bayesian_model) |&gt; plot()\n\n\n\n\n\n\n\npd(Bayesian_model) |&gt; plot()\n\n\n\n\n\n\n\nrope(Bayesian_model, range = c(-3.03, 3.03))|&gt;plot()\n\n\n\n\n\n\n\n# hdi: Based on the Bayesian analysis, the true difference between groups is likely to be within the interval [-2.10, 1.19] with a 95% probability.\n# The interval includes values that span both negative and positive differences, suggesting that there might be non-significant differences between the groups.\n# pd: The probability that the true difference is negative or positive is 69.73%.\n# There is unclear support for the true difference being in either the negative or positive direction, but the probability is slightly higher for a negative difference.\n# rope: The rope is defined as [-3.03, 3.03].\n# It represents a range where differences within this interval are considered practically equivalent or negligible.\n# 100% of the posterior distribution falls within this area, indicating strong support for the practical equivalence of the groups.\n# Cohen's d Effect Size:\neffectsize::effectsize(Bayesian_model) \n\nCohen's d |        95% CI\n-------------------------\n-0.01     | [-0.07, 0.04]\n\n- Estimated using pooled SD.\n\n# The estimated Cohen's d is -0.01 with a 95% CI [-0.07, 0.04], indicating a negligible negative effect size and no statistically significant difference between the groups.\n\nThe Second Question\nA two-way ANOVA was conducted to examine the effects of Obesity and Smoking status on Cholesterol levels. Obesity had two levels (Not Obese/Obese), and Smoking status also had two levels (Doesn’t Smoke/Smokes). We found no significant main effect for Obesity [F(1, 4996)=2.11, p=0.146] and no significant main effect for Smoking status [F(1, 4996)=0.58, p=.446]. We found a significant interaction effect between Obesity and Smoking status on Cholesterol levels [F(1, 4996)=4.73, p=0.03], suggesting that the relationship between Obesity and Cholesterol varies depending on an individual’s Smoking status.\nFurther examination of this significant interaction effect using simple effect analysis revealed significant differences in Cholesterol levels between obese and non-obese individuals among non-smokers [contrast estimate=3.11, SE=1.21, t(4996)=2.57, p=.01], indicating higher Cholesterol levels among non-smoking non-obese individuals compared to their obese counterparts. This difference was not significant among smokers [contrast estimate=-0.62, SE=1.22, t(4996)=.5, p=.61]. The interaction effect is illustrated in the code’s output.\n\n#anova\nanova&lt;-aov_ez(id = \"PatientID\", dv = \"Cholesterol\",\n                       between = c(\"Obesity\", \"Smoking\"),\n                       data = data,\n                       anova_table = list(es = \"pes\")) \nanova\n\nAnova Table (Type 3 tests)\n\nResponse: Cholesterol\n           Effect      df    MSE      F   pes p.value\n1         Obesity 1, 4996 917.99   2.11 &lt;.001    .146\n2         Smoking 1, 4996 917.99   0.58 &lt;.001    .446\n3 Obesity:Smoking 1, 4996 917.99 4.73 * &lt;.001    .030\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\neta_squared(anova, partial = TRUE)\n\n# Effect Size for ANOVA (Type III)\n\nParameter       | Eta2 (partial) |       95% CI\n-----------------------------------------------\nObesity         |       4.22e-04 | [0.00, 1.00]\nSmoking         |       1.16e-04 | [0.00, 1.00]\nObesity:Smoking |       9.47e-04 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n#exploring the significant interaction\nggemmeans(anova, c(\"Obesity\", \"Smoking\"))\n\n# Predicted values of Cholesterol\n\nSmoking: 0\n\nObesity | Predicted |         95% CI\n------------------------------------\n0       |    201.63 | 199.99, 203.28\n1       |    198.52 | 196.82, 200.22\n\nSmoking: 1\n\nObesity | Predicted |         95% CI\n------------------------------------\n0       |    199.11 | 197.40, 200.82\n1       |    199.73 | 198.07, 201.40\n\njoint_tests(anova, by = \"Smoking\")\n\nSmoking = 0:\n model term df1  df2 F.ratio p.value\n Obesity      1 4996   6.630  0.0101\n\nSmoking = 1:\n model term df1  df2 F.ratio p.value\n Obesity      1 4996   0.259  0.6106\n\nem_Obesity_by_Smoking &lt;- emmeans(anova, ~ Obesity + Smoking)\nem_Obesity_by_Smoking\n\n Obesity Smoking emmean    SE   df lower.CL upper.CL\n 0       0          202 0.840 4996      200      203\n 1       0          199 0.869 4996      197      200\n 0       1          199 0.872 4996      197      201\n 1       1          200 0.849 4996      198      201\n\nConfidence level used: 0.95 \n\nc_simpeff &lt;- contrast(em_Obesity_by_Smoking, method = \"pairwise\", by = \"Smoking\")\nc_simpeff\n\nSmoking = 0:\n contrast            estimate   SE   df t.ratio p.value\n Obesity0 - Obesity1     3.11 1.21 4996   2.575  0.0101\n\nSmoking = 1:\n contrast            estimate   SE   df t.ratio p.value\n Obesity0 - Obesity1    -0.62 1.22 4996  -0.509  0.6106\n\n#let's plot\nem_df &lt;- as.data.frame(em_Obesity_by_Smoking)\nggplot(em_df, aes(x = Obesity, y = emmean, color = Smoking)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width = 0.2) +\n  geom_line(aes(group = Smoking), linetype = \"dashed\") +\n  labs(x = \"Obesity Status\", y = \"Estimated Marginal Means of Cholesterol\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) + \n  scale_x_discrete(labels = c(\"0\" = \"Not Obese\", \"1\" = \"Obese\")) +\n  scale_color_discrete(labels = c(\"0\" = \"Doesn't Smoke\", \"1\" = \"Smokes\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe Third Question\nA multiple linear regression analysis was conducted to investigate the relationship between Cholesterol levels and two predictor variables: BMI and Exercise Hours Per Week. The regression model yielded a significant overall relationship [F(2, 4997)=4.425, p=.01]. However, the model’s explanatory power was limited, with an adjusted R-squared of .001, indicating that only approximately 0.1% of the variance in Cholesterol levels could be explained by BMI and Exercise Hours Per Week. The individual predictors were examined further and indicated that Exercise Hours Per Week was a significant predictor [β=1, 95% C.I. (0.17, 1.84), p=0.019], but BMI was not [β=.15, 95% C.I. (-0.02, 0.32), p=0.07]. The model is illustrated in the code’s output.\n\nfit&lt;- lm(Cholesterol ~ BMI + ExerciseHours , data = data)\nsummary(fit)\n\n\nCall:\nlm(formula = Cholesterol ~ BMI + ExerciseHours, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-116.10  -20.22   -0.33   20.96  118.29 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   193.01798    2.49378  77.400   &lt;2e-16 ***\nBMI             0.15090    0.08539   1.767   0.0773 .  \nExerciseHours   1.00471    0.42786   2.348   0.0189 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.29 on 4997 degrees of freedom\nMultiple R-squared:  0.001768,  Adjusted R-squared:  0.001368 \nF-statistic: 4.425 on 2 and 4997 DF,  p-value: 0.01202\n\nmodel_parameters(fit)\n\nParameter     | Coefficient |   SE |           95% CI | t(4997) |      p\n------------------------------------------------------------------------\n(Intercept)   |      193.02 | 2.49 | [188.13, 197.91] |   77.40 | &lt; .001\nBMI           |        0.15 | 0.09 | [ -0.02,   0.32] |    1.77 | 0.077 \nExerciseHours |        1.00 | 0.43 | [  0.17,   1.84] |    2.35 | 0.019 \n\nmodel_parameters(fit, standardize = \"basic\") \n\nParameter     | Std. Coef. |   SE |        95% CI | t(4997) |      p\n--------------------------------------------------------------------\n(Intercept)   |       0.00 | 0.00 | [ 0.00, 0.00] |   77.40 | &lt; .001\nBMI           |       0.02 | 0.01 | [ 0.00, 0.05] |    1.77 | 0.077 \nExerciseHours |       0.03 | 0.01 | [ 0.01, 0.06] |    2.35 | 0.019 \n\nmodel_performance(fit)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n-----------------------------------------------------------------------\n48303.042 | 48303.050 | 48329.110 | 0.002 |     0.001 | 30.282 | 30.291\n\n#let's plot the model\ndata$ExerciseHours &lt;- as.numeric(as.character(data$ExerciseHours)) #converting to numeric so we don't run into errors\nggplot(data, aes(y = Cholesterol, x = ExerciseHours, color = BMI)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Relationship Between Cholesterol, BMI, and Exercise Hours\",\n       x = \"Exercise Hours\",\n       y = \"Cholesterol\",\n       color = \"BMI\") +\n  scale_color_gradient(low = \"blue\", high = \"pink\") + \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n# Analysis of the standardized coefficients:\n# ExerciseHours has a standardized coefficient of 0.03. P-value &lt; 0.05.\n# BMI has a standardized coefficient of 0.02. P-value &gt; 0.05\n# Therefore, based on the standardized coefficients:\n# ExerciseHours has a larger standardized contribution to predicting cholesterol levels compared to BMI.\n# Conclusion: ExerciseHours shows a statistically significant positive association with cholesterol levels, whereas BMI does not.\n\nThe Fourth Question\nThe moderation analysis examined the moderating effect of Exercise Hours Per Week on the relationship between Family History and Cholesterol levels. The outcome variable for analysis was Cholesterol levels, the predictor variable was Family History, and the moderator was Exercise Hours Per Week. The model yielded an overall significant relationship, despite a low explanatory power [F(3, 4996)=2.63, p=0.04, adjusted R2=.0015]. The interaction between Family History and Exercise Hours Per Week was not significant [β=-1.15, 95% C.I. (-2.82, 0.53), p=0.18]. Further examination of the predictors revealed that Exercise Hours Per Week was a significant predictor [β=1.6, 95% C.I. (0.41, 2.79), p=.009], but Family History was not [β=3.95, 95% C.I. (-1.33, 9.23), p=.142]. These results overall suggest that Exercise Hours Per Week is not a moderator of the relationship between Family History and Cholesterol levels.\nThe additive model was explored to compare it to the moderative model. The regression analysis revealed that the model is overall significant, despite a low explanatory power [F(2, 4997)=3.05, p=0.04, adjusted R2=.0008]. Further exploration showed that Exercise Hours Per Week was a significant predictor [β=1.02, 95% C.I. (0.18, 1.86), p=0.017], but Family History was not [β=0.53, 95% C.I. (-1.15, 2.21), p=0.533].\nResults of the model comparison analysis (between the moderative and additive model) showed that the additive model, while penalizing for model complexity, is a better fit to the data [BIC=48331.8, RMSE=30.29, R2=.001] than the moderative model [BIC=48338.6, RMSE=30.28, R2=.002]. However, this is a relatively small difference given the explanatory power of each model. For each of the moderative and additive models, we split the data into training and test sets. The MSE results for each model’s training and testing sets are shown in the code’s output.\nAs we can see in Figure 3 below, the test set of both the moderation and additive models has a higher MSE value than the training set. This indicates a pattern of overfitting, where the model performs better on the training set (lower MSE) than on the testing set (higher MSE).\n\nm_additive &lt;- lm(Cholesterol ~ FamilyHistory + ExerciseHours,\n                 data = data)\nm_moderation &lt;- lm(Cholesterol ~ FamilyHistory * ExerciseHours,\n                   data = data)\n#let's check model parameters and trends\nsummary(m_moderation)\n\n\nCall:\nlm(formula = Cholesterol ~ FamilyHistory * ExerciseHours, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-117.577  -20.158   -0.386   21.042  119.018 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 194.7482     1.9065 102.148  &lt; 2e-16 ***\nFamilyHistory                 3.9542     2.6931   1.468  0.14209    \nExerciseHours                 1.5994     0.6077   2.632  0.00852 ** \nFamilyHistory:ExerciseHours  -1.1463     0.8557  -1.340  0.18043    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.3 on 4996 degrees of freedom\nMultiple R-squared:  0.00158,   Adjusted R-squared:  0.0009808 \nF-statistic: 2.636 on 3 and 4996 DF,  p-value: 0.04807\n\nmodel_parameters(m_moderation)\n\nParameter                     | Coefficient |   SE |           95% CI | t(4996) |      p\n----------------------------------------------------------------------------------------\n(Intercept)                   |      194.75 | 1.91 | [191.01, 198.49] |  102.15 | &lt; .001\nFamilyHistory                 |        3.95 | 2.69 | [ -1.33,   9.23] |    1.47 | 0.142 \nExerciseHours                 |        1.60 | 0.61 | [  0.41,   2.79] |    2.63 | 0.009 \nFamilyHistory × ExerciseHours |       -1.15 | 0.86 | [ -2.82,   0.53] |   -1.34 | 0.180 \n\nemtrends(m_moderation, ~ ExerciseHours, var = \"FamilyHistory\")\n\n ExerciseHours FamilyHistory.trend    SE   df lower.CL upper.CL\n          2.98               0.534 0.857 4996    -1.15     2.21\n\nResults are averaged over the levels of: FamilyHistory \nConfidence level used: 0.95 \n\nmodel_performance(m_moderation)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n-----------------------------------------------------------------------\n48305.981 | 48305.993 | 48338.567 | 0.002 | 9.808e-04 | 30.285 | 30.297\n\n# Explaining additional model performance:\n# AIC=  48303.042\n# AIC is a measure used to compare the goodness of fit of the statistical model, balancing fit with model complexity.\n# The large AIC value suggests that the model is less likely to be the best model to explain the variance in cholesterol levels.\n# BIC= 48329.110\n# BIC is similar to AIC but places a stronger penalty on models with more parameters.\n# The large BIC value indicates that the model does not fit the data well.\nsummary(m_additive)\n\n\nCall:\nlm(formula = Cholesterol ~ FamilyHistory + ExerciseHours, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-116.597  -20.078   -0.453   21.125  117.900 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   196.4682     1.4095 139.385   &lt;2e-16 ***\nFamilyHistory   0.5341     0.8570   0.623    0.533    \nExerciseHours   1.0212     0.4279   2.387    0.017 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.3 on 4997 degrees of freedom\nMultiple R-squared:  0.001222,  Adjusted R-squared:  0.0008219 \nF-statistic: 3.056 on 2 and 4997 DF,  p-value: 0.04716\n\nmodel_parameters(m_additive)\n\nParameter     | Coefficient |   SE |           95% CI | t(4997) |      p\n------------------------------------------------------------------------\n(Intercept)   |      196.47 | 1.41 | [193.70, 199.23] |  139.38 | &lt; .001\nFamilyHistory |        0.53 | 0.86 | [ -1.15,   2.21] |    0.62 | 0.533 \nExerciseHours |        1.02 | 0.43 | [  0.18,   1.86] |    2.39 | 0.017 \n\n#now let's compare the models\ncompare_performance(m_additive, m_moderation)\n\n# Comparison of Model Performance Indices\n\nName         | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) |    R2 | R2 (adj.) |   RMSE |  Sigma\n----------------------------------------------------------------------------------------------------------------\nm_additive   |    lm | 48305.8 (0.526) | 48305.8 (0.526) | 48331.8 (0.966) | 0.001 | 8.219e-04 | 30.291 | 30.300\nm_moderation |    lm | 48306.0 (0.474) | 48306.0 (0.474) | 48338.6 (0.034) | 0.002 | 9.808e-04 | 30.285 | 30.297\n\n#----\n#training and test\nset.seed(123)\nindex &lt;- sample(1:nrow(data), nrow(data) * 0.7) #70:30 partition\ntraining_data &lt;- data[index, ]\ntest_data &lt;- data[-index, ]\nfit_additive&lt;-lm(Cholesterol ~ FamilyHistory + ExerciseHours,\n         data = training_data)\npredictions_additive &lt;- predict(fit_additive, newdata = test_data)\nfit_moderation&lt;-lm(Cholesterol ~ FamilyHistory * ExerciseHours,\n                   data = training_data)\npredictions_moderation&lt;-predict(fit_moderation, newdata = test_data)\n#let's calculate MSEs\n#for additive model\nresiduals_additive_train &lt;- training_data$Cholesterol - predict(fit_additive, newdata = training_data)\nresiduals_additive_test &lt;- test_data$Cholesterol - predict(fit_additive, newdata = test_data)\nmse_additive_train &lt;- mean(residuals_additive_train^2)\nmse_additive_train\n\n[1] 905.6822\n\nmse_additive_test &lt;- mean(residuals_additive_test^2)\nmse_additive_test\n\n[1] 945.3417\n\n#for moderation model\nresiduals_moderation_train &lt;- training_data$Cholesterol - predict(fit_moderation, newdata = training_data)\nresiduals_moderation_test &lt;- test_data$Cholesterol - predict(fit_moderation, newdata = test_data)\nmse_moderation_train &lt;- mean(residuals_moderation_train^2)\nmse_moderation_train\n\n[1] 904.5554\n\nmse_moderation_test &lt;- mean(residuals_moderation_test^2)\nmse_moderation_test\n\n[1] 947.6777\n\n#let's create a barchart for MSEs\n# first, let's create a df\nmse_data &lt;- data.frame(\n  Model = c(\"Additive Model (Training)\", \"Additive Model (Test)\", \n            \"Moderation Model (Training)\", \"Moderation Model (Test)\"),\n  MSE = c(905.6822, 945.3417, 904.5554, 947.6777), #these are the results respectively\n  Type = c(\"Training\", \"Test\", \"Training\", \"Test\")\n)\n#and now let's plot MSEs\nggplot(mse_data, aes(x = Model, y = MSE, fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  labs(title = \"Mean Squared Error (MSE) Comparison\",\n       x = \"Model\", y = \"MSE\") +\n  scale_fill_manual(values = c(\"Training\" = \"blue\", \"Test\" = \"green\")) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  }
]